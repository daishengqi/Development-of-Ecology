{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec trainning\n",
    "Considering that our corpus is relatively small, we chose 100 dimension vectors for word presentation, Both CBOW and Skip-gram models are trainned, negative sampling is used to accelerate. The trainning algorithm is deployed under the framework of gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sentences = ReloadCorpus('D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_TiAb_BiReplaced.txt')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :%(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(Sentences, size = 100, min_count = 3, sg = 1, hs = 1)\n",
    "model.save('D:/_Research/Project_Ecological_Development/Data_Processing/SkipGram_HierSoftmax.model')\n",
    "\n",
    "model = gensim.models.Word2Vec(Sentences, size = 100, min_count = 3, sg = 0, hs = 0)\n",
    "model.save('D:/_Research/Project_Ecological_Development/Data_Processing/CBOW_NegativeSampling.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concept word group generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, delete all the stopwords in both corpuses using nltk.stopwords.words('english'), then lemmatize the corpus using nltk.WordNetLemaatizer (Attention, this step takes long time and delete stopwords first will accelerate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function Declaration\n",
    "def DelStopwords(data, UniBi): #For stopwords deletion in Corpus files\n",
    "    stpwds = stopwords.words('english')    \n",
    "    result = []\n",
    "    if UniBi == 'Sent':\n",
    "        for record in data:\n",
    "            result.append([word for word in record if word not in stpwds])\n",
    "    if UniBi == 'Uni':\n",
    "        result = [word for word in data if word not in stpwds]\n",
    "    return(result)\n",
    "\n",
    "def Lemm(data, ps):#Data must be whole corpus, contains mutiple word Lists [[x,x,x],[x,x,x],...,[x,x,x]]Style\n",
    "    result = []\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    if ps == 'v' or ps == 'n':\n",
    "        for record in data:\n",
    "            result.append([str(wnl.lemmatize(word, pos = ps)) for word in record]) #POS = 'v' make this lemmatizer process verbs and 'n' for nouns, Recommend both.\n",
    "    if ps == 'all':\n",
    "        for record in data:\n",
    "            tempresult = []\n",
    "            tempresult = [str(wnl.lemmatize(word, pos = 'n')) for word in record]\n",
    "            result.append([str(wnl.lemmatize(word, pos = 'v')) for word in tempresult]) #Make both pos lemmatized\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stopwords deletion\n",
    "Title_Corpus = DelStopwords(Title_Corpus, 'Sent')\n",
    "Abstract_Corpus = DelStopwords(Abstract_Corpus, 'Sent')\n",
    "\n",
    "#Lemmalitization\n",
    "Title_Corpus = Lemm(Title_Corpus, 'all')\n",
    "Abstract_Corpus = Lemm(Abstract_Corpus, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this contemporary files into local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputFiles(data, FileDirectory, UniBi): #For Signle word and Sentence Outputing\n",
    "    Fileoutput = open(FileDirectory, 'w', encoding = u'utf-8')\n",
    "    if UniBi == 'Uni':\n",
    "        for record in data:\n",
    "            print(record, file = Fileoutput)\n",
    "    if UniBi == 'Bi':\n",
    "        for record in data:\n",
    "            print(record[0], record[1], file = Fileoutput)    \n",
    "    if UniBi == 'Sent':\n",
    "        for record  in data:\n",
    "            print(Connect(record), file = Fileoutput)\n",
    "    Fileoutput.close()\n",
    "    \n",
    "#Output\n",
    "OutputFiles(Title_Corpus,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Title_Lemmstp.txt','Sent')\n",
    "OutputFiles(Abstract_Corpus,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Abstract_Lemmstp.txt','Sent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bigram selection and corpus refining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this step, we'll refining the corpus by selecting important bigrams using TF-IDF, and then replace them back into the original corpus, Top 1000 bigrams are selected after mannual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the function declaration\n",
    "def CombineBigrams(Bgrams):\n",
    "    result = []\n",
    "    for sentence in Bgrams:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            temp.append((word[0] + '-' + word[1]))\n",
    "        if temp != []:\n",
    "            result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def ReturnTfidfRank(Sentences,Top):\n",
    "    dictionary = gensim.corpora.Dictionary(Sentences)\n",
    "    corpus = [dictionary.doc2bow(sentence) for sentence in Sentences]\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    TokenList = []\n",
    "    for document in corpus:\n",
    "        if document != []:\n",
    "            TokenList.append(str(dictionary.get(sorted(tfidf[document],key = lambda record: record[1], reverse = True)[0][0])))\n",
    "    result = [word for word in Sort_Rebuild(FreqDist(TokenList))][0:Top]\n",
    "    return(result)\n",
    "\n",
    "def Sort_Rebuild(Freq):#Rebuild and Sort FreqDist files, make it easier to check\n",
    "    Freq_Rebuild = [[word,Freq[word]] for word in Freq]\n",
    "    Freq_Sorted = sorted(Freq_Rebuild, key = lambda record: record[1], reverse = True)\n",
    "    return(Freq_Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the title and abstract corpus\n",
    "Lemm_Sentences = Title_Corpus + Abstract_Corpus\n",
    "\n",
    "#Make Bigrams\n",
    "Bgrams = []\n",
    "for sentence in Lemm_Sentences:\n",
    "    Bgrams.append(list(nltk.bigrams(sentence)))\n",
    "\n",
    "#Chosing using TF-IDF\n",
    "Bgrams = CombineBigrams(Bgrams)\n",
    "Bgram_Top_List = ReturnTfidfRank(Bgrams, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output these Top 1000 Bigrams into local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bgram_Top_List = [word[0] for word in Bgram_Top]\n",
    "OutputFiles(Bgram_Top_List,'D:/_Research/Project_Ecological_Development/Data_Processing/Top1000_Bgrams_TFIDF.txt','Uni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace bigrams back into the corpus using loop, Here, we changed the loop style into 'temp in Bgram_Top_List', which made it nearly 10000 times faster than original compare by new hash table comparing.\n",
    "Attention, the time consumed by this step is mainly controlled by the volume of Bgram_Top_List, at the complexity of O(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram replacement\n",
    "C_Lemm_Sentences = Lemm_Sentences\n",
    "for i in range(0,len(Lemm_Sentences)):\n",
    "    for j in range (0,len(Lemm_Sentences[i])-1):\n",
    "        temp = Lemm_Sentences[i][j] + '-' + Lemm_Sentences[i][j+1]\n",
    "        if (temp in Bgram_Top_List):\n",
    "            C_Lemm_Sentences[i][j] = temp\n",
    "            C_Lemm_Sentences[i][j+1] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputFiles(C_Lemm_Sentences,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_TiAb_BiReplaced.txt','Sent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
