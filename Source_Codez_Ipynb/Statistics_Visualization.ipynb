{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedicated statistics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Figure 1 Configuration\n",
    "Data for both panels of figure 1 is shown here, while the visualization is proceeded by Microsofit Excel and Sci2:\n",
    "### 1.1 Figure 1(a) - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "Records = Records[Records.DT == 'Article']\n",
    "PubYear = np.array(Records.PY)\n",
    "\n",
    "#Extract Reprint author's nationality\n",
    "Digremove = str.maketrans('0123456789',',,,,,,,,,,')\n",
    "#Replace RP with C1 if RP is absent\n",
    "Temp = []\n",
    "for (index, item) in enumerate(Records.iterrows()):\n",
    "    if item[1].RP != item[1].RP: #Incase the Reprint author is empty\n",
    "        if item[1].C1 != item[1].C1: #Incase the author address is empty\n",
    "            Temp.append(str(item[1].PA))\n",
    "        else:\n",
    "            Temp.append(str(item[1].C1).split('|')[0])\n",
    "    else:\n",
    "        Temp.append(item[1].RP)\n",
    "        \n",
    "RAU_Country = [str(author).lower().translate(Digremove).split(',')[-1].replace(' ','').replace('.','') for author in Temp]\n",
    "RAU_Country = np.array(RAU_Country)\n",
    "#Replace all double codes to usa\n",
    "RAU_Country[[len(item)<3 for item in RAU_Country]] = 'usa'\n",
    "RAU_Country[[item[-3:] == 'usa' for item in RAU_Country]] = 'usa'\n",
    "Countries = pd.value_counts(RAU_Country).index\n",
    "\n",
    "#Create Matrix\n",
    "Year_Country = pd.DataFrame()\n",
    "for topcountry in Countries:\n",
    "    Year_Country.insert(0,topcountry,[(topcountry == country) for country in RAU_Country])\n",
    "Year_Country.insert(0,'Year',PubYear)\n",
    "Year_Country_Table = Year_Country.groupby('Year').sum()\n",
    "Year_Country_Table.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Top20Country_PerYear_20171216.csv', encoding = u'utf-8')\n",
    "#Output all countires\n",
    "Year_Country_Table.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/AllCountry_PerYear_20180319.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Figure 1(b) - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Records = Records.assign(Country = RAU_Country)\n",
    "#Take top 10 publication countries and count their top journals\n",
    "country = Countries[10]\n",
    "total = sum(pd.value_counts(Records[Records.Country == country].SO)[:5])\n",
    "ToptenSO = pd.DataFrame(pd.value_counts(Records[Records.Country == country].SO)[:5]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Figure 2 Configuration\n",
    "This figure is composed by python, below we will show the data preparation and visualziation code:\n",
    "### 2.1 Figure 2(a) & (b) - Data preparation\n",
    "Concept Relative importance calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "#For Ti + Ab Please read this line\n",
    "ConInPaper_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_TiAb_20171129.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "#For Ti only Please read this line\n",
    "ConInPaper_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_Ti_20171216.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "\n",
    "ConInPaper_Matrix = ConInPaper_Matrix.drop('Unnamed: 0', axis = 1)\n",
    "PubYear = np.array(Records.PY)\n",
    "ConInPaper_Matrix.insert(0,'Year',PubYear)\n",
    "del(Records)\n",
    "\n",
    "#Per_Year_Total calculation -----New\n",
    "Per_Year_Total = ConInPaper_Matrix.groupby('Year').sum()\n",
    "\n",
    "#Concept Coverage\n",
    "ConInPaper_Matrix = ConInPaper_Matrix.drop('Year', axis = 1)\n",
    "Concept_Coverage = [False for i in range(len(ConInPaper_Matrix))]\n",
    "for concept in ConInPaper_Matrix:\n",
    "    Concept_Coverage = MatrixCombine(Concept_Coverage, ConInPaper_Matrix[concept])\n",
    "print(sum(Concept_Coverage))\n",
    "    \n",
    "#For Ti + Ab output\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_PerYear_TiAb_20171204.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_PerYear_Ti_20171216.csv', encoding = u'utf-8')\n",
    "\n",
    "#PerYear_RI calculation --------- New --------- Will raise changes in Per_Year_Total\n",
    "temp = Per_Year_Total.T\n",
    "for line in temp:\n",
    "    temp[line] = temp[line] / sum(temp[line])\n",
    "Per_Year_RI = temp.T\n",
    "\n",
    "#For Ti+Ab output\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RI_TiAb_PerYear_20171205.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RI_Ti_PerYear_20171205.csv', encoding = u'utf-8')\n",
    "\n",
    "#Reset the Per_Year_Total for latter use\n",
    "Per_Year_Total = ConInPaper_Matrix.groupby('Year').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category Relative importancecalculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConInPaper_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_TiAb_20171129.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "#For Ti only Please read this line\n",
    "ConInPaper_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_Ti_20171216.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "\n",
    "\n",
    "#Prepare the category dictionary\n",
    "Categories = [item.split('|') for item in Waiting_List.Category]\n",
    "for cate in Categories:\n",
    "    try:\n",
    "        cate.remove('')\n",
    "    except:\n",
    "        print('None')\n",
    "Waiting_List.insert(0,'Categories',Categories)\n",
    "\n",
    "temp = []\n",
    "for item in Categories:\n",
    "    temp += item\n",
    "    \n",
    "Cate_dict = {}\n",
    "for cate in pd.value_counts(temp).index:\n",
    "    Cate_dict.update({cate:[]})\n",
    "\n",
    "for concept in Waiting_List.iterrows():\n",
    "    for cate in concept[1].Categories:\n",
    "        Cate_dict[cate].append(concept[1].Concepts)\n",
    "\n",
    "#Generate Year_Category\n",
    "Year_Category = pd.DataFrame()\n",
    "for cate in Cate_dict:\n",
    "    temp = [False for i in range(len(ConInPaper_Matrix))]\n",
    "    print(cate)\n",
    "    for concept in Cate_dict[cate]:\n",
    "        temp = MatrixCombine(temp, ConInPaper_Matrix[concept])\n",
    "    Year_Category.insert(0,cate,temp)\n",
    "    \n",
    "#For Ti + Ab output\n",
    "Year_Category.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_TiAb_20171201.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Year_Category.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_Ti_20171217.csv', encoding = u'utf-8')\n",
    "\n",
    "\n",
    "\n",
    "#Category statistics\n",
    "#For TI+AB\n",
    "Category_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_TiAb_20171201.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "#For TI only\n",
    "Category_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_Ti_20171217.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "\n",
    "Category_Matrix = Category_Matrix.drop('Unnamed: 0', axis = 1)\n",
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "PubYear = np.array(Records.PY)\n",
    "del(Records)\n",
    "\n",
    "#Per_Year_Total calculation -----New\n",
    "Category_Matrix.insert(0,'Year',PubYear)\n",
    "Per_Year_Total = Category_Matrix.groupby('Year').sum()\n",
    "\n",
    "#For Ti + Ab\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_PerYear_TiAb_20171204.csv', encoding = u'utf-8')\n",
    "#For Ti only\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_PerYear_Ti_20171217.csv', encoding = u'utf-8')\n",
    "\n",
    "\n",
    "#PerYear_RI calculation --------- New --------- Will raise changes in Per_Year_Total\n",
    "temp = Per_Year_Total.T\n",
    "for line in temp:\n",
    "    temp[line] = temp[line] / sum(temp[line])\n",
    "Per_Year_RI = temp.T\n",
    "\n",
    "#For Ti + Ab\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_RI_PerYear_TiAb_20171205.csv', encoding = u'utf-8')\n",
    "#For Ti only\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_RI_PerYear_Ti_20171217.csv', encoding = u'utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Figure 2 - Visualziation\n",
    "Both panels of Figure 2 used similar codez, packaged as a function later. Here we first showed the codez for Figure 2(a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConcpetRI_data = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RI_TiAb_PerYear_20171205.csv',\n",
    "                            encoding = u'utf-8').set_index('Year').stack()\n",
    "ConcpetRI_data.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RIStacked_TiAb_PerYear_20171205.csv')\n",
    "\n",
    "Paintdata = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RIStacked_TiAb_PerYear_20171205.csv',encoding = u'utf-8').sort_values(by = 'Cate')\n",
    "\n",
    "Selected_Concepts = np.array(Paintdata.groupby('Cate').sum().sort_values('RI_value', ascending = False).index[:15])\n",
    "Selected_Concepts[4] = 'Ecosystem'\n",
    "Paintdata = Paintdata[[item in Selected_Concepts for item in Paintdata.Cate]]\n",
    "\n",
    "%matplotlib qt5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "#Set the RI to percentage according to the paper\n",
    "Paintdata.RI_value *= 100\n",
    "df = Paintdata\n",
    "Con_fontsize = 12\n",
    "\n",
    "# Initialize a grid of plots with an Axes for each walk\n",
    "pal = sns.dark_palette(\"purple\", n_colors = 15, reverse=True)\n",
    "#pal = sns.cubehelix_palette(16, rot=-.25, light=.7)\n",
    "grid = sns.FacetGrid(df, col=\"Cate\", hue=\"Cate\", col_wrap=5, size=2.5, palette = pal, legend_out = True)\n",
    "#grid = sns.FacetGrid(df, row=\"Cate\", hue=\"Cate\", aspect = 1, size=5, palette = pal)\n",
    "    \n",
    "# Draw a line plot to show the trajectory of each random walk\n",
    "#grid.map(plt.plot, \"Year\", \"RI_value\", marker=\"o\", ms=4)\n",
    "grid.map(sns.regplot,\"Year\",\"RI_value\", line_kws = {\"linewidth\":0.5}, scatter_kws = {\"marker\":\".\", \"s\":10})\n",
    "# Draw a horizontal line to show the starting point\n",
    "grid.map(plt.axhline, y=0, ls=\":\", c=\".5\")\n",
    "# Adjust the tick positions and labels\n",
    "grid.set(xticks=np.arange(1915,2016,25), yticks=np.arange(-10,50,5),\n",
    "         xlim=(1915, 2015), ylim=(-5, 20))\n",
    "\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.5,1.05, label, fontsize = Con_fontsize, fontweight=\"bold\", color=color, \n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "grid.map(label, \"Cate\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "#grid.fig.subplots_adjust(wspace=.05, hspace=.05)\n",
    "\n",
    "grid.set_xlabels(\"Year\", fontsize = Con_fontsize, fontweight=\"bold\")\n",
    "grid.set_xticklabels(fontsize = Con_fontsize)\n",
    "\n",
    "grid.set_ylabels(\"Relative Importance(%)\", fontsize = Con_fontsize, fontweight=\"bold\")\n",
    "grid.set_yticklabels(fontsize = Con_fontsize)\n",
    "grid.fig.tight_layout(w_pad=1)\n",
    "grid.set_titles(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composition of Figure 2(b) is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CateRITI_data = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_RI_PerYear_Ti_20171217.csv',\n",
    "                            encoding = u'utf-8').set_index('Year').stack()\n",
    "CateRITI_data.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/test.csv')\n",
    "Paintdata = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Cate_Test.csv',encoding = u'utf-8').sort_values(by = 'Cate')\n",
    "\n",
    "%matplotlib qt5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "#Set the RI to percentage according to the paper\n",
    "Paintdata.RI_value *= 100\n",
    "df = Paintdata\n",
    "Con_fontsize = 12\n",
    "\n",
    "# Initialize a grid of plots with an Axes for each walk\n",
    "pal = sns.dark_palette(\"purple\", n_colors = 13, reverse=True)\n",
    "#pal = sns.cubehelix_palette(13, start = 2.8, rot = .1, light = .7)\n",
    "grid = sns.FacetGrid(df, col=\"Cate\", hue=\"Cate\", col_wrap = 5, size=2.5, palette = pal, legend_out = True)\n",
    "#grid = sns.FacetGrid(df, row=\"Cate\", hue=\"Cate\", aspect = 1, size=5, palette = pal)\n",
    "    \n",
    "# Draw a line plot to show the trajectory of each random walk\n",
    "#grid.map(plt.plot, \"Year\", \"RI_value\", marker=\"o\", ms=4)\n",
    "grid.map(sns.regplot,\"Year\",\"RI_value\", line_kws = {\"linewidth\":0.5}, scatter_kws = {\"marker\":\".\", \"s\":10})\n",
    "# Draw a horizontal line to show the starting point\n",
    "grid.map(plt.axhline, y=0, ls=\":\", c=\".5\")\n",
    "# Adjust the tick positions and labels\n",
    "grid.set(xticks=np.arange(1915,2016,25), yticks=np.arange(-10,50,5),\n",
    "         xlim=(1915, 2015), ylim=(-5, 40))\n",
    "\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.5, 1.05, label, fontsize = Con_fontsize, fontweight=\"bold\", color=color, \n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "grid.map(label, \"Cate\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "#grid.fig.subplots_adjust(wspace=.05, hspace=.05)\n",
    "grid.set_xlabels(\"Year\", fontsize = Con_fontsize, fontweight=\"bold\")\n",
    "grid.set_xticklabels(fontsize = Con_fontsize)\n",
    "grid.set_ylabels(\"Relative Importance(%)\", fontsize = Con_fontsize, fontweight=\"bold\")\n",
    "grid.set_yticklabels(fontsize = Con_fontsize)\n",
    "grid.fig.tight_layout(w_pad=1)\n",
    "grid.set_titles(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Figure 3 Configuration\n",
    "We finished Figure 3 using both python and Adobe illustrator, here we'll show the codez:\n",
    "### 3.1 Figure 3 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the calculation of Preference Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Reload Category info\n",
    "Category_Matrix = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_TiAb_20171201.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "Category_Matrix = Category_Matrix.drop('Unnamed: 0', axis = 1)\n",
    "Category_Matrix.insert(0,'Country',RAU_Country)\n",
    "Per_Country_Total = Category_Matrix.groupby('Country').sum()\n",
    "Per_Country_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Country_Category_20180320.csv', encoding = u'utf-8')\n",
    "\n",
    "#######Painting\n",
    "PaintMatrix = Per_Country_Total.sort_values('Species', ascending = False)[:30].T.sort_values('usa', ascending = False)[:30]\n",
    "#Proportion Calculation\n",
    "for item in PaintMatrix:\n",
    "    PaintMatrix[item] = PaintMatrix[item]/sum(PaintMatrix[item])\n",
    "\n",
    "#Average Calculation\n",
    "AveragePro = [np.mean(item[1]) for item in PaintMatrix.iterrows()]\n",
    "HueAverage = pd.DataFrame()\n",
    "for item in PaintMatrix:\n",
    "    HueAverage.insert(0, item, AveragePro)\n",
    "HueAverage = HueAverage.set_index(PaintMatrix.index)\n",
    "\n",
    "#Calculate Residue of countries, showed up by change percentage\n",
    "Residue = pd.DataFrame()\n",
    "for item in PaintMatrix:\n",
    "    Residue.insert(0, item, (PaintMatrix[item] - HueAverage[item]) / HueAverage[item])\n",
    "Residue = Residue.set_index(PaintMatrix.index)\n",
    "\n",
    "PaintMatrix.stack().to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Country_Top10_Concept_Fig4_20180323.csv', encoding = u'utf-8')\n",
    "HueAverage.stack().to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Country_Top10_Concept_Average_20180323.csv', encoding = u'utf-8')\n",
    "Residue.stack().to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Country_Top10_Concept_Residue_20180326.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Figure 3 - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paintdata = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Country_Concept_Top30_Residue_20180326.csv', encoding = u'utf-8')\n",
    "Country = Paintdata.Country\n",
    "Concept = Paintdata.Concept\n",
    "Paintdata.Percentage *= 100\n",
    "Paintdata = Paintdata.pivot(\"Country\", \"Concept\", \"Percentage\").T\n",
    "#Paintdata = Paintdata.sort_values(by = \"Species\").T.sort_values(by = \"USA\")\n",
    "%matplotlib qt5\n",
    "\n",
    "sns.set()\n",
    "sns.set(font = \"Arial\",font_scale = 1.5)\n",
    "# Generate a custom diverging colormap or try \"RdBu_r\"\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Create a categorical palette to identify the networks\n",
    "network_pal = sns.husl_palette(30, s = .45)\n",
    "Concept_lut = dict(zip(Paintdata.index, network_pal))\n",
    "Country_lut = dict(zip(Paintdata.T.index, network_pal))\n",
    "\n",
    "# Convert the palette to vectors that will be drawn on the side of the matrix\n",
    "Concept_colors = pd.Series(Paintdata.index, index = Paintdata.index).map(Concept_lut)\n",
    "Country_colors = pd.Series(Paintdata.T.index, index = Paintdata.T.index).map(Country_lut)\n",
    "\n",
    "ax = sns.clustermap(Paintdata, cmap = cmap, method = \"average\",\n",
    "               row_colors=Concept_colors, col_colors=Country_colors,\n",
    "               vmax = 120, vmin = -100, center = 0, linewidth = 1, figsize=(15, 15),\n",
    "               cbar_kws = {\"label\":\"Percentage Index(%)\"})\n",
    "plt.setp(ax.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "plt.setp(ax.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Figure 4 Configuration\n",
    "We configured Figure 4 using Microsoft Excel and Adobe illustrator, the codez for data preparation is already shown in Section 1.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
