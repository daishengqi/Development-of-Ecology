{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model trainning & Ecological concepts(categories) clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition and library import for this section, please run before progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition and library import\n",
    "import gensim, logging\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.probability import FreqDist #Frequency Library for Freqdist\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#Function Definintion\n",
    "def BreakintoCorpus(data,minwordlen):#Delete punctuation and short words, split sentence into Corpus Lists\n",
    "    result = [word.lower() for word in Delpunc(data).split(' ') if len(word) > minwordlen]\n",
    "    return(result)\n",
    "\n",
    "def CombineBigrams(Bgrams):#Combine [x,x] Bigram to 'x-x' style\n",
    "    result = []\n",
    "    for sentence in Bgrams:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            temp.append((word[0] + '-' + word[1]))\n",
    "        if temp != []:\n",
    "            result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def CombineTrigrams(Tgrams):#Same as above\n",
    "    result = []\n",
    "    for sentence in Tgrams:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            temp.append((word[0] + '-' + word[1] + '-' + word[2]))\n",
    "        if temp != []:\n",
    "            result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def Connect(Array):#For standard output\n",
    "    string = ''\n",
    "    for word in Array:\n",
    "        string = string + word + ' '\n",
    "    return(string)\n",
    "\n",
    "def Delpunc(kstring):#Accepting STRINGS for puncutation deletion\n",
    "    for punc in punctuation:\n",
    "        kstring = kstring.replace(punc,' ')\n",
    "    return(kstring)\n",
    "\n",
    "def DelStopwords(data, UniBi): #For stopwords deletion in Corpus files\n",
    "    #For excat term \"Bottom-up and Top-down\"\n",
    "    stpwds = stopwords.words('english')\n",
    "    stpwds.remove('up')\n",
    "    stpwds.remove('down')\n",
    "    \n",
    "    result = []\n",
    "    if UniBi == 'Sent':\n",
    "        for record in data:\n",
    "            result.append([word for word in record if word not in stpwds])\n",
    "    if UniBi == 'Uni':\n",
    "        result = [word for word in data if word not in stpwds]\n",
    "    return(result)\n",
    "\n",
    "def Lemm(data, ps):#Data must be whole corpus, contains mutiple word Lists [[x,x,x],[x,x,x],...,[x,x,x]]Style\n",
    "    result = []\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    if ps == 'v' or ps == 'n':\n",
    "        for record in data:\n",
    "            result.append([str(wnl.lemmatize(word, pos = ps)) for word in record]) #POS = 'v' make this lemmatizer process verbs and 'n' for nouns, Recommend both.\n",
    "    if ps == 'all':\n",
    "        for record in data:\n",
    "            tempresult = []\n",
    "            tempresult = [str(wnl.lemmatize(word, pos = 'n')) for word in record]\n",
    "            result.append([str(wnl.lemmatize(word, pos = 'v')) for word in tempresult]) #Make both pos lemmatized\n",
    "    return(result)\n",
    "\n",
    "def MatrixCombine(matrix,cmatrix): #Calculate the point-point OR result for two boolean array\n",
    "    #For OR calculation using +, for AND calculation using *\n",
    "    return(np.array(matrix) + np.array(cmatrix))\n",
    "\n",
    "def OutputFiles(data, FileDirectory, UniBi): #For Signle word and Sentence Outputing\n",
    "    Fileoutput = open(FileDirectory, 'w', encoding = u'utf-8')\n",
    "    if UniBi == 'Uni':\n",
    "        for record in data:\n",
    "            print(record, file = Fileoutput)\n",
    "    if UniBi == 'Bi':\n",
    "        for record in data:\n",
    "            print(record[0], record[1], file = Fileoutput)    \n",
    "    if UniBi == 'Sent':\n",
    "        for record  in data:\n",
    "            print(Connect(record), file = Fileoutput)\n",
    "    Fileoutput.close()\n",
    "\n",
    "def PaperFreq(Word, Corpus): #Return the boolean Matrix of Word in Corpus\n",
    "    if Word != '':\n",
    "        matrix = [(Word in paper) for paper in Corpus]\n",
    "    else:\n",
    "        matrix = [False for i in range(len(Corpus))]\n",
    "    return(matrix)\n",
    "    \n",
    "def ReloadCorpus(FileDirectory): #For Single Word and Sentence Reloading\n",
    "    Fileinput = open(FileDirectory, encoding = u'utf-8')\n",
    "    result = []\n",
    "    for line in Fileinput:\n",
    "        result.append(line.replace('\\n','').split())\n",
    "    Fileinput.close()\n",
    "    return(result)\n",
    "\n",
    "def ReturnTfidfRank(Sentences,Top):\n",
    "    dictionary = gensim.corpora.Dictionary(Sentences)\n",
    "    corpus = [dictionary.doc2bow(sentence) for sentence in Sentences]\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    TokenList = []\n",
    "    for document in corpus:\n",
    "        if document != []:\n",
    "            TokenList.append(str(dictionary.get(sorted(tfidf[document],key = lambda record: record[1], reverse = True)[0][0])))\n",
    "    result = [word for word in Sort_Rebuild(FreqDist(TokenList))][0:Top]\n",
    "    return(result)\n",
    "\n",
    "def Sort_Rebuild(Freq):#Rebuild and Sort FreqDist files, make it easier to check\n",
    "    Freq_Rebuild = [[word,Freq[word]] for word in Freq]\n",
    "    Freq_Sorted = sorted(Freq_Rebuild, key = lambda record: record[1], reverse = True)\n",
    "    return(Freq_Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec trainning\n",
    "Considering that our corpus is relatively small, we chose 100 dimension vectors for word presentation, Both CBOW and Skip-gram models are trainned, negative sampling is used to accelerate. The trainning algorithm is deployed under the framework of gensim, minimum threshold of word frequency is set to 1, for our corpus is relatively small, it's a waste to drop too much words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sentences = ReloadCorpus('D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_TiAb_BiReplaced.txt')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :%(message)s', level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec(Sentences, size = 100, min_count = 1, sg = 1, hs = 1)\n",
    "model.save('D:/_Research/Project_Ecological_Development/Data_Processing/SkipGram_HierSoftmax.model')\n",
    "\n",
    "model = gensim.models.Word2Vec(Sentences, size = 100, min_count = 1, sg = 0, hs = 1)\n",
    "model.save('D:/_Research/Project_Ecological_Development/Data_Processing/CBOW_HierSoftmax.model') #For our corpus, CBOW and HierSoftmax is better. This is the model used in the future processing\n",
    "\n",
    "model = gensim.models.Word2Vec(Sentences, size = 100, min_count = 1, sg = 0, hs = 0)\n",
    "model.save('D:/_Research/Project_Ecological_Development/Data_Processing/CBOW_NegativeSampling.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cluster generation on ecological concepts & categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mannual preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of concept words are prepared mannually, each candidate concept will be deliverred into the trained Word2Vec model for similarity comparison, which will generate word cluster on particular ecological concept.\n",
    "\n",
    "However, the mannual preparation of candidate words is critical but somehow vague. We state our algorithm of candidate selection here: 1) For unigram concepts, we directly deliver the unigram itself as candidate. 2) For bigram concepts, only slash divided unigram is deliverred, e.g. edge-effect. We can promise these concepts themselves are absolutely available in the word list, for we've already added all these candidates into wordlist in the last processing step. 3) For n-grams (n>=3), concept terms are further transformed to bigrams or even unigrams, e.g. 'population growth mode' are divided into population-growth and growth-mode, then treated as two bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For similarity comparison algorithm of concept cluster generation, it is calculated by model.wv.most_similar() in package gensim. This method computes cosine similarity between a simple mean of the projection weight vectors of the given words and the vectors for each word in the model. Further for quantification, we choose the parameter topN as 20 for future cluster formation, which means there are at least 20 candidate words for each concept to generate concept cluster. We aslo attached the word similarity value to the result as resident for mannual selection. Here we'll import our model and read man-made import words for clustering search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention: Ended at 16th Jan. 2018, there are still TWO mannual operations above, ONE as Concept Candidate Generation and the OTHER as Concept Cluster Definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################Word Clustering###########################################\n",
    "model = gensim.models.Word2Vec.load('D:/_Research/Project_Ecological_Development/Data_Processing/CBOW_HierSoftmax.model')\n",
    "Waiting_List = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_InputWords_20171129.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search each word for similiar words in building appropriate candidate word groups for concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Candidates = []\n",
    "for words in Waiting_List.Input_Word:\n",
    "    result = []\n",
    "    temp = words.split('|')\n",
    "    for word in temp:\n",
    "        result.extend(model.wv.most_similar(word, topn = 20))\n",
    "    #Sort the candidates by similarity\n",
    "    result = sorted(np.array(result), key = lambda record: record[1], reverse = True)\n",
    "    Candidates.append([str(item[0]) for item in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save these candidates to local disk for mannual Concept Cluster Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print candidates for mannual filtering\n",
    "Waiting_List.insert(3,'Candidates',Candidates)\n",
    "Waiting_List.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Categlory_Candidates_20171129.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data preparation and Concept(Category) Tagging\n",
    "The corpus for concept tagging was changed through the pushing of the research of this project. Till 16th Jan. 2018, the throughout analysis of concepts during 1915 to 2015 has been restricted to article titles (TI) only, more precise research during 1991 to 2015 is set to titles and abstract(TI + AB). However, at code level, we'll tag all the data of both TI and TI+AB, the furhter research diverses will only affect the cut of result on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Selected Frequency#######################################\n",
    "Waiting_List = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Selected_20171129.csv', encoding = u'utf-8')\n",
    "Waiting_List = Waiting_List.drop('Unnamed: 0',axis = 1)\n",
    "Sentences = ReloadCorpus('D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_TiAb_MannualBiReplaced.txt')\n",
    "Selected_Frequency = [item.split(',') for item in Waiting_List.Selected_20171129.fillna('')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentences original file is well devided into two parts:first half as titles (Complete) and the other half as abstracts (Absence possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure the distribution of selected Concepts in the Total Corpus for 1991~2015 research\n",
    "PaperLen = int(len(Sentences)/2)\n",
    "Title_Abstract = []\n",
    "for index in range(PaperLen):\n",
    "    Title_Abstract.append(Sentences[index]+Sentences[index + PaperLen])\n",
    "    \n",
    "#The corpus of titles for 100 year statistics\n",
    "Titles = Sentences[:PaperLen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using defined functions to count the existence of agents in both datasets(TI and TI + AB).\n",
    "#### Attention: till 6th Jan. 2018, the counting function has been refined to the third version (much faster), which used matrix level logical reasoning and calculation. So the result will be in the form of bollean (True & False), DO NOT CHANGE IT INTO 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the existence of agents in Titles + Abstracts\n",
    "ConceptMatrix_TiAb = []\n",
    "for Concept in Selected_Frequency:\n",
    "    Matrix = [False for i in range(len(Title_Abstract))]\n",
    "    for Word in Concept:\n",
    "        Matrix = MatrixCombine(Matrix, PaperFreq(Word, Title_Abstract))\n",
    "    ConceptMatrix_TiAb.append(Matrix)\n",
    "    #print(Concept)\n",
    "\n",
    "#Count the existence of agents in Titles\n",
    "ConceptMatrix_Ti = []\n",
    "for Concept in Selected_Frequency:\n",
    "    Matrix = [False for i in range(len(Titles))]\n",
    "    for Word in Concept:\n",
    "        Matrix = MatrixCombine(Matrix, PaperFreq(Word, Titles))\n",
    "    ConceptMatrix_Ti.append(Matrix)\n",
    "    #print(Concept)\n",
    "\n",
    "#Statistics\n",
    "Total_TiAb = [sum(concept) for concept in ConceptMatrix_TiAb]\n",
    "Total_Ti = [sum(concept) for concept in ConceptMatrix_Ti]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the result matrix and transform it into pd.DataFrame. Then export the result to Localdisk, with column names inverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For matrix on TI+AB\n",
    "ConInPaper_Matrix = pd.DataFrame(np.matrix(ConceptMatrix_TiAb).T, columns = Waiting_List.Concepts)\n",
    "ConInPaper_Matrix.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_TiAb_20171129.csv', encoding = u'utf-8')\n",
    "#For matrix on TI only\n",
    "ConInPaper_Matrix = pd.DataFrame(np.matrix(ConceptMatrix_Ti).T, columns = Waiting_List.Concepts)\n",
    "ConInPaper_Matrix.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_Ti_20171216.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Concept mapping and foundamental statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping every concept and category based on the occurence matrix progressed in the last step. First, reload the data based on the request and insert the publication year into matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publishment reload\n",
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "PubYear = np.array(Records.PY)\n",
    "del(Records)\n",
    "\n",
    "#For Ti + Ab Please read this line\n",
    "ConInPaper_Matrix_TiAb = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_TiAb_20171129.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "#For Ti only Please read this line\n",
    "ConInPaper_Matrix_Ti = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_Ti_20171216.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "\n",
    "ConInPaper_Matrix_TiAb.insert(0,'Year',PubYear)\n",
    "ConInPaper_Matrix_Ti.insert(0,'Year',PubYear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the matrix by year using built-in algorithm of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per_Year_Total calculation -----New\n",
    "Per_Year_Total_TiAb = ConInPaper_Matrix_TiAb.groupby('Year').sum()\n",
    "Per_Year_Total_Ti = ConInPaper_Matrix_Ti.groupby('Year').sum()\n",
    "\n",
    "#Concept Coverage Calculation\n",
    "ConInPaper_Matrix = ConInPaper_Matrix.drop('Year', axis = 1)\n",
    "Concept_Coverage = [False for i in range(len(ConInPaper_Matrix))]\n",
    "for concept in ConInPaper_Matrix:\n",
    "    Concept_Coverage = MatrixCombine(Concept_Coverage, ConInPaper_Matrix[concept])\n",
    "print(sum(Concept_Coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Ti + Ab output\n",
    "Per_Year_Total_TiAb.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_PerYear_TiAb_20171204.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Per_Year_Total_Ti.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_PerYear_Ti_20171216.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tagging and counting the absolute existence of concepts in both datasets, we'll try figure out the relative importance of each concepts. For relative importance, we'll devide the count of concepts each year with the amount of all concepts in that particular year.\n",
    "\n",
    "While the matrix itself is not in the right direction, we'll tranpose it to calculation and tranpose back to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PerYear_RI calculation --------- New --------- Will raise changes in Per_Year_Total\n",
    "temp = Per_Year_Total.T\n",
    "for line in temp:\n",
    "    temp[line] = temp[line] / sum(temp[line])\n",
    "Per_Year_RI = temp.T\n",
    "\n",
    "#For Ti+Ab output\n",
    "Per_Year_RI_TiAb.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RI_TiAb_PerYear_20171205.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Per_Year_RI_Ti.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_RI_Ti_PerYear_20171205.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Category mapping and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecological cateogry is composed of different ecological concepts, which requires combination of matrix and cluster for further statistics. The original data is prepared in the Wating_List of mannual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure the distribution of selected Categories in the corpus\n",
    "Waiting_List = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Selected_20171129.csv', encoding = u'utf-8')\n",
    "#For Ti + Ab Please read this line\n",
    "ConInPaper_Matrix_TiAb = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_TiAb_20171129.csv', false_values = ['0'], encoding = u'utf-8')\n",
    "#For Ti only Please read this line\n",
    "ConInPaper_Matrix_Ti = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Concept_Paper_Ti_20171216.csv', false_values = ['0'], encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the category dictionary\n",
    "Categories = [item.split('|') for item in Waiting_List.Category]\n",
    "for cate in Categories:\n",
    "    try:\n",
    "        cate.remove('')\n",
    "    except:\n",
    "        print('None')\n",
    "Waiting_List.insert(0,'Categories',Categories)\n",
    "\n",
    "temp = []\n",
    "for item in Categories:\n",
    "    temp += item\n",
    "    \n",
    "Cate_dict = {}\n",
    "for cate in pd.value_counts(temp).index:\n",
    "    Cate_dict.update({cate:[]})\n",
    "\n",
    "for concept in Waiting_List.iterrows():\n",
    "    for cate in concept[1].Categories:\n",
    "        Cate_dict[cate].append(concept[1].Concepts)\n",
    "\n",
    "#Generate Year_Category\n",
    "Year_Category = pd.DataFrame()\n",
    "for cate in Cate_dict:\n",
    "    temp = [False for i in range(len(ConInPaper_Matrix))]\n",
    "    print(cate)\n",
    "    for concept in Cate_dict[cate]:\n",
    "        temp = MatrixCombine(temp, ConInPaper_Matrix[concept])\n",
    "    Year_Category.insert(0,cate,temp)\n",
    "    \n",
    "#For Ti + Ab output\n",
    "Year_Category.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_TiAb_20171201.csv', encoding = u'utf-8')\n",
    "#For Ti output\n",
    "Year_Category.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_Paper_Ti_20171217.csv', encoding = u'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundamental category based statistics, including absolute counting and relative importance, which is similar to concept statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "PubYear = np.array(Records.PY)\n",
    "del(Records)\n",
    "\n",
    "#Per_Year_Total calculation -----New\n",
    "Category_Matrix.insert(0,'Year',PubYear)\n",
    "Per_Year_Total = Category_Matrix.groupby('Year').sum()\n",
    "\n",
    "#For Ti + Ab\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_PerYear_TiAb_20171204.csv', encoding = u'utf-8')\n",
    "#For Ti only\n",
    "Per_Year_Total.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_PerYear_Ti_20171217.csv', encoding = u'utf-8')\n",
    "\n",
    "#PerYear_RI calculation --------- New --------- Will raise changes in Per_Year_Total\n",
    "temp = Per_Year_Total.T\n",
    "for line in temp:\n",
    "    temp[line] = temp[line] / sum(temp[line])\n",
    "Per_Year_RI = temp.T\n",
    "\n",
    "#For Ti + Ab\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_RI_PerYear_TiAb_20171205.csv', encoding = u'utf-8')\n",
    "#For Ti only\n",
    "Per_Year_RI.to_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Category_RI_PerYear_Ti_20171217.csv', encoding = u'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
