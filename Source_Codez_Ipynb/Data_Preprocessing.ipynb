{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Break interested contents into Corpus\n",
    "In this research, the fields that we interested in were Titles, Abstracts, Keywords and WOS Keywords. Extract these fields into corpus by dividing them using space and lower all the words, moreover, the punctuations were deleted in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import *\n",
    "\n",
    "def delpunc(kstring):#Accepting STRINGS for puncutation deletion\n",
    "    for punc in punctuation:\n",
    "        kstring = kstring.replace(punc,' ')\n",
    "    return(kstring)\n",
    "\n",
    "def BreakintoCorpus(data,minwordlen): #Delete punctuation and short words, split sentence into Corpus Lists\n",
    "    result = [word.lower() for word in delpunc(data).split(' ') if len(word) > minwordlen]\n",
    "    return(result)\n",
    "\n",
    "Records = pd.read_csv('D:/_Research/Project_Ecological_Development/Data_Processing/Complete_Records_WOS_20171111.csv', encoding = u'utf-8')\n",
    "Records = Records.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "Title_Corpus = [BreakintoCorpus(title,0) for title in Records.TI]\n",
    "Abstract_Corpus = [BreakintoCorpus(str(abstract),0) for abstract in Records.AB] #str(abstracts) in case of NaN in most abstracts\n",
    "Keyword_Corpus = [str(keyword).split('; ') for keyword in Records.DE]\n",
    "WOSword_Corpus = [str(keyword).split('; ') for keyword in Records.ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lemmatization and Stopword deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, delete all the stopwords in both corpuses using nltk.stopwords.words('english'), then lemmatize the corpus using nltk.WordNetLemaatizer (Attention, this step takes long time and delete stopwords first will accelerate this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: special words like 'up' and 'down' will be restored, for they are useful in latter searches (\"Bottom-up\" and \"Top-down\" control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function Declaration\n",
    "def DelStopwords(data, UniBi): #For stopwords deletion in Corpus files\n",
    "    #For excat term \"Bottom-up and Top-down\"\n",
    "    stpwds = stopwords.words('english')\n",
    "    stpwds.remove('up')\n",
    "    stpwds.remove('down')\n",
    "    \n",
    "    result = []\n",
    "    if UniBi == 'Sent':\n",
    "        for record in data:\n",
    "            result.append([word for word in record if word not in stpwds])\n",
    "    if UniBi == 'Uni':\n",
    "        result = [word for word in data if word not in stpwds]\n",
    "    return(result)\n",
    "\n",
    "def Lemm(data, ps):#Data must be whole corpus, contains mutiple word Lists [[x,x,x],[x,x,x],...,[x,x,x]]Style\n",
    "    result = []\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    if ps == 'v' or ps == 'n':\n",
    "        for record in data:\n",
    "            result.append([str(wnl.lemmatize(word, pos = ps)) for word in record]) #POS = 'v' make this lemmatizer process verbs and 'n' for nouns, Recommend both.\n",
    "    if ps == 'all':\n",
    "        for record in data:\n",
    "            tempresult = []\n",
    "            tempresult = [str(wnl.lemmatize(word, pos = 'n')) for word in record]\n",
    "            result.append([str(wnl.lemmatize(word, pos = 'v')) for word in tempresult]) #Make both pos lemmatized\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stopwords deletion\n",
    "Title_Corpus = DelStopwords(Title_Corpus, 'Sent')\n",
    "Abstract_Corpus = DelStopwords(Abstract_Corpus, 'Sent')\n",
    "\n",
    "#Lemmalitization\n",
    "Title_Corpus = Lemm(Title_Corpus, 'all')\n",
    "Abstract_Corpus = Lemm(Abstract_Corpus, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this contemporary files into local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputFiles(data, FileDirectory, UniBi): #For Signle word and Sentence Outputing\n",
    "    Fileoutput = open(FileDirectory, 'w', encoding = u'utf-8')\n",
    "    if UniBi == 'Uni':\n",
    "        for record in data:\n",
    "            print(record, file = Fileoutput)\n",
    "    if UniBi == 'Bi':\n",
    "        for record in data:\n",
    "            print(record[0], record[1], file = Fileoutput)    \n",
    "    if UniBi == 'Sent':\n",
    "        for record  in data:\n",
    "            print(Connect(record), file = Fileoutput)\n",
    "    Fileoutput.close()\n",
    "    \n",
    "#Output\n",
    "OutputFiles(Title_Corpus,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Title_Lemmstp.txt','Sent')\n",
    "OutputFiles(Abstract_Corpus,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Abstract_Lemmstp.txt','Sent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bigram,Trigram selection and corpus refining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this step, we'll refining the corpus by selecting important bigrams and trigrams using TF-IDF, and then replace them back into the original corpus.\n",
    "Top 10000 grams were chosen and saved to local disk, future replacement will be deployed inside these lists.\n",
    "Attention: considering the requirement of word indexing in continuous work, bigger n-grams are not calculated for their are way too time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the function declaration\n",
    "def CombineBigrams(Bgrams):\n",
    "    result = []\n",
    "    for sentence in Bgrams:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            temp.append((word[0] + '-' + word[1]))\n",
    "        if temp != []:\n",
    "            result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def CombineTrigrams(Tgrams):\n",
    "    result = []\n",
    "    for sentence in Tgrams:\n",
    "        temp = []\n",
    "        for word in sentence:\n",
    "            temp.append((word[0] + '-' + word[1] + '-' + word[2]))\n",
    "        if temp != []:\n",
    "            result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def ReturnTfidfRank(Sentences,Top):\n",
    "    dictionary = gensim.corpora.Dictionary(Sentences)\n",
    "    corpus = [dictionary.doc2bow(sentence) for sentence in Sentences]\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    TokenList = []\n",
    "    for document in corpus:\n",
    "        if document != []:\n",
    "            TokenList.append(str(dictionary.get(sorted(tfidf[document],key = lambda record: record[1], reverse = True)[0][0])))\n",
    "    result = [word for word in Sort_Rebuild(FreqDist(TokenList))][0:Top]\n",
    "    return(result)\n",
    "\n",
    "def Sort_Rebuild(Freq):#Rebuild and Sort FreqDist files, make it easier to check\n",
    "    Freq_Rebuild = [[word,Freq[word]] for word in Freq]\n",
    "    Freq_Sorted = sorted(Freq_Rebuild, key = lambda record: record[1], reverse = True)\n",
    "    return(Freq_Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corpus data for replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_Corpus = ReloadCorpus('D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Title_Lemmstp.txt')\n",
    "Abstract_Corpus = ReloadCorpus('D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_Abstract_Lemmstp.txt')\n",
    "#Combine the title and abstract corpus\n",
    "Lemm_Sentences = Title_Corpus + Abstract_Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test, we'll trill replace the trigrams first, save top 10000 grams to local disk. THIS IS JUST A TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram = []\n",
    "for sentence in Lemm_Sentences:\n",
    "    Trigram.append(list(nltk.trigrams(sentence)))\n",
    "    \n",
    "Trigram = CombineTrigrams(Trigram)\n",
    "Trigram_Top = ReturnTfidfRank(Trigram, 10000)\n",
    "Trigram_Top_List = [word[0] for word in Trigram_Top]\n",
    "OutputFiles(Trigram_Top_List,'D:/_Research/Project_Ecological_Development/Data_Processing/Top10000_Trigrams_TFIDF.txt','Uni')\n",
    "\n",
    "Trigram_Top_List = [word[0] for word in Trigram_Top][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are actually processed in our research, they are important and crutial in latter analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Bigrams\n",
    "Bgrams = []\n",
    "for sentence in Lemm_Sentences:\n",
    "    Bgrams.append(list(nltk.bigrams(sentence)))\n",
    "\n",
    "#Chosing using TF-IDF\n",
    "Bgrams = CombineBigrams(Bgrams)\n",
    "Bgram_Top_List = ReturnTfidfRank(Bgrams, 10000)\n",
    "Bgram_Top_List = [word[0] for word in Bgram_Top]\n",
    "OutputFiles(Bgram_Top_List,'D:/_Research/Project_Ecological_Development/Data_Processing/Top10000_Bgrams_TFIDF.txt','Uni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace bigrams back into the corpus using loop, Here, we changed the loop style into 'temp in Bgram_Top_List', which made it nearly 10000 times faster than original compare by new hash table comparing.\n",
    "Attention, the time consumed by this step is mainly controlled by the volume of Bgram_Top_List, at the complexity of O(n). Here we choose 2000 as the threshold after mannual inspectation of the list.\n",
    "#### Here, for better defeined ECOLOGICAL bigram in word meaning, we manually checked the input words and added some bigrams that mentioned in the later concept word table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mannual added bigrams \n",
    "Mannual_Bigrams = ['birth-rate','competitive-exclusion','energy-efficiency','founder-effect','intermediate-disturbance','intrinsic-grwoth','lotka-volterra',\n",
    "                   'nutrient-cycle','red-queen','specie-evenness','secondary-productivity','doubling-time','nutrient-pool','nutrient-turnover','energy-flow',\n",
    "                   'equilibrium-model','evolutionary-divergence','evolutionary-convergence','exponential-growth','exponential-increase','hardy-weinberg',\n",
    "                   'intrinsic-increase','k-r','nutrient-spiraling','patch-matrix','selection-type','secondary-productivity','specie-area','specie-category',\n",
    "                   'trophic-pyramid','density-independent','residence-time','island-biogeography','faciliation-model','tolerance-model','inhibition-model',\n",
    "                   'competition-model','stable-state','ecosystem-production','biogeochemical-cycle','assemblage-richness','assemblage-evenness','specie-dispersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Bgram_Top' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ae209d1d744b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBgram_Top_List\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBgram_Top\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Bigram replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mC_Lemm_Sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemm_Sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLemm_Sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLemm_Sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Bgram_Top' is not defined"
     ]
    }
   ],
   "source": [
    "Bgram_Top_List = [word[0] for word in Bgram_Top][:2000] + Mannual_Bigrams\n",
    "#Bigram replacement\n",
    "C_Lemm_Sentences = Lemm_Sentences\n",
    "for i in range(0,len(Lemm_Sentences)):\n",
    "    for j in range (0,len(Lemm_Sentences[i])-1):\n",
    "        temp = Lemm_Sentences[i][j] + '-' + Lemm_Sentences[i][j+1]\n",
    "        if (temp in Bgram_Top_List):\n",
    "            C_Lemm_Sentences[i][j] = temp\n",
    "            C_Lemm_Sentences[i][j+1] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputFiles(C_Lemm_Sentences,'D:/_Research/Project_Ecological_Development/Data_Processing/Sentences_TiAb_BiReplaced.txt','Sent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
